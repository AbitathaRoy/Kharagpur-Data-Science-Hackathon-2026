{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be70a1d3",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pathway and heavy dependencies\n",
    "# We use 'udiff' to enable the computation engine\n",
    "%pip install pathway -q\n",
    "%pip install sentence-transformers -q\n",
    "%pip install openai -q\n",
    "%pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8884f",
   "metadata": {},
   "source": [
    "# Step By Step Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868bde5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Kharagpur-Data-Science-Hackathon-2026' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AbitathaRoy/Kharagpur-Data-Science-Hackathon-2026.git\n",
    "# Then adjust your paths like: CHUNKS_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/chunks.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7559b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "  > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const version = '3.7.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n  const BK_RE = /^https:\\/\\/cdn\\.bokeh\\.org\\/bokeh\\/(release|dev)\\/bokeh-/;\n  const PN_RE = /^https:\\/\\/cdn\\.holoviz\\.org\\/panel\\/[^/]+\\/dist\\/panel/i;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, Bokeh, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'tabulator': 'https://cdn.jsdelivr.net/npm/tabulator-tables@6.3.1/dist/js/tabulator.min', 'moment': 'https://cdn.jsdelivr.net/npm/luxon/build/global/luxon.min'}, 'shim': {}});\n      require([\"tabulator\"], function(Tabulator) {\n        window.Tabulator = Tabulator\n        on_load()\n      })\n      require([\"moment\"], function(moment) {\n        window.moment = moment\n        on_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.Tabulator !== undefined) && (!(window.Tabulator instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.8.4/dist/bundled/datatabulator/tabulator-tables@6.3.1/dist/js/tabulator.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    if (((window.moment !== undefined) && (!(window.moment instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.8.4/dist/bundled/datatabulator/luxon/build/global/luxon.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      const shouldSkip = skip.includes(escaped) || existing_scripts.includes(escaped)\n      const isBokehOrPanel = BK_RE.test(escaped) || PN_RE.test(escaped)\n      const missingOrBroken = Bokeh == null || Bokeh.Panel == null || (Bokeh.version != version && !Bokeh.versions?.has(version)) || Bokeh.versions?.get(version).Panel == null;\n      if (shouldSkip && !(isBokehOrPanel && missingOrBroken)) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.4/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.holoviz.org/panel/1.8.4/dist/bundled/datatabulator/tabulator-tables@6.3.1/dist/js/tabulator.min.js\", \"https://cdn.holoviz.org/panel/1.8.4/dist/bundled/datatabulator/luxon/build/global/luxon.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.holoviz.org/panel/1.8.4/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [\"https://cdn.holoviz.org/panel/1.8.4/dist/bundled/datatabulator/tabulator-tables@6.3.1/dist/css/tabulator_simple.min.css\"];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false;\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true;\n      root._bokeh_onload_callbacks = [];\n      const bokeh_loaded = Bokeh != null && ((Bokeh.version === version && Bokeh.Panel) || (Bokeh.versions?.has(version) && Bokeh.versions.get(version).Panel));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, Bokeh, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n        if (Bokeh != undefined && !reloading) {\n          const NewBokeh = root.Bokeh;\n          if (Bokeh.versions === undefined) {\n            Bokeh.versions = new Map();\n          }\n          if (NewBokeh.version !== Bokeh.version) {\n            Bokeh[NewBokeh.version] = NewBokeh;\n            Bokeh.versions.set(NewBokeh.version, NewBokeh);\n          }\n          root.Bokeh = Bokeh;\n        }\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='85882478-3b5c-4d4c-af6d-52f9aa9301d1'>\n",
       "  <div id=\"fdee312e-0649-4f37-8b21-4dd2331db2e6\" data-root-id=\"85882478-3b5c-4d4c-af6d-52f9aa9301d1\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"655c5d6e-2972-4b89-a88f-448d914dead3\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"85882478-3b5c-4d4c-af6d-52f9aa9301d1\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"8b7aaad8-f836-4746-9025-85f412129959\",\"attributes\":{\"plot_id\":\"85882478-3b5c-4d4c-af6d-52f9aa9301d1\",\"comm_id\":\"f2b5fce82941463fb8d4dd23f7c1cf84\",\"client_comm_id\":\"02c18e7797514d56b26ec183ed347681\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"start\",\"kind\":\"Any\",\"default\":0},{\"name\":\"end\",\"kind\":\"Any\",\"default\":100},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"655c5d6e-2972-4b89-a88f-448d914dead3\",\"roots\":{\"85882478-3b5c-4d4c-af6d-52f9aa9301d1\":\"fdee312e-0649-4f37-8b21-4dd2331db2e6\"},\"root_ids\":[\"85882478-3b5c-4d4c-af6d-52f9aa9301d1\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(version);\n",
       "    } else if (root.Bokeh.version === version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.Tabulator !== undefined) && ( root.Tabulator !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "85882478-3b5c-4d4c-af6d-52f9aa9301d1"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pathway as pw\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_IMPLICIT_TOKEN\"] = \"1\" # Stops the HF Token warning\n",
    "\n",
    "# Load from environment or .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Configuration\n",
    "CHUNKS_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/chunks.jsonl\"\n",
    "CLAIMS_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/claims_output.json\"\n",
    "OUTPUT_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/final_predictions_pathway.jsonl\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Detection Prompt (The \"Detective\" Persona)\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "You are a Detective verifying a witness statement against the official case files.\n",
    "\n",
    "WITNESS CLAIM: \"{claim}\"\n",
    "\n",
    "OFFICIAL CASE FILES (NOVEL TEXT):\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Search the Case Files for the specific events/characters mentioned in the Claim.\n",
    "2. **Compare Details:** Look closely at names, dates, causes of death, and relationships.\n",
    "3. **Verdict Logic:**\n",
    "   - **CONTRADICT**: If the text explicitly tells a *different* story (e.g., Claim says \"Shot\", Text says \"Stabbed\").\n",
    "   - **CONTRADICT**: If the Claim says X happened, but the Text says Y happened *instead*.\n",
    "   - **CONSISTENT**: If the Claim is supported by the text.\n",
    "   - **CONSISTENT**: If the Claim adds extra details that do *not* conflict with the text (Silence is not a lie).\n",
    "\n",
    "Return JSON: {{ \"verdict\": \"consistent\" | \"contradict\", \"reason\": \"Citing specific passage [x]\" }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8f0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "@pw.udf\n",
    "def call_judge_model(claim: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls Groq/OpenAI to judge the consistency.\n",
    "    \"\"\"\n",
    "    # Initialize client inside the UDF for pickling reasons in distributed mode\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://api.groq.com/openai/v1\",\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    formatted_prompt = JUDGE_PROMPT.format(claim=claim, context=context)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\", # Or \"qwen/qwen3-32b\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a ruthless logic engine. Output strict JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"verdict\": \"error\", \"reason\": str(e)})\n",
    "\n",
    "@pw.udf\n",
    "def parse_verdict(llm_response: str) -> str:\n",
    "    try:\n",
    "        data = json.loads(llm_response)\n",
    "        return data.get(\"verdict\", \"consistent\")\n",
    "    except:\n",
    "        return \"consistent\"\n",
    "\n",
    "@pw.udf\n",
    "def parse_reason(llm_response: str) -> str:\n",
    "    try:\n",
    "        data = json.loads(llm_response)\n",
    "        return data.get(\"reason\", \"Parse Error\")\n",
    "    except:\n",
    "        return \"Parse Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a93ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't use 'id' as a column name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-923250849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m claims_with_vectors = claims.with_columns(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mbook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"book_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/trace.py\u001b[0m in \u001b[0;36m_pathway_trace_marker\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0m_reraise_with_user_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pathway_trace_marker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/trace.py\u001b[0m in \u001b[0;36m_reraise_with_user_frame\u001b[0;34m(e, trace)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_pathway_trace_note\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/desugaring.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             }\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/table.py\u001b[0m in \u001b[0;36mwith_columns\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtrace_user_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/trace.py\u001b[0m in \u001b[0;36m_pathway_trace_marker\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0m_reraise_with_user_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pathway_trace_marker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/trace.py\u001b[0m in \u001b[0;36m_reraise_with_user_frame\u001b[0;34m(e, trace)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0madd_pathway_trace_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/desugaring.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             }\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/arg_handlers.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return G.add_operator(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moperator_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/parse_graph.py\u001b[0m in \u001b[0;36madd_operator\u001b[0;34m(self, create_node, call_operator, special, require_error_log)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_error_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# deferred global log creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_error_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_log_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrequire_error_log\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munused_operators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/decorators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(operator)\u001b[0m\n\u001b[1;32m     22\u001b[0m         return G.add_operator(\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moperator_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/operator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_arg_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/table.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mnew_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mall_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_args_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/desugaring.py\u001b[0m in \u001b[0;36mcombine_args_kwargs\u001b[0;34m(args, kwargs, exclude_columns)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pathway/internals/desugaring.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(name, expression)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Duplicate expression value given for {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't use 'id' as a column name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumnExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mexpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumnConstExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't use 'id' as a column name"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Ingest Data\n",
    "# We read the JSONL files into Pathway Tables\n",
    "documents = pw.io.jsonlines.read(\n",
    "    CHUNKS_FILE,\n",
    "    schema=pw.schema_from_dict({\n",
    "        \"chunk_text\": str,\n",
    "        \"metadata\": dict  # We need metadata.book_name\n",
    "    }),\n",
    "    mode=\"static\" # Static mode for batch processing\n",
    ")\n",
    "\n",
    "claims = pw.io.jsonlines.read(\n",
    "    CLAIMS_FILE,\n",
    "    schema=pw.schema_from_dict({\n",
    "        \"id\": str,\n",
    "        \"input_text\": str,\n",
    "        \"metadata\": dict # We need metadata.book_name here too\n",
    "    }),\n",
    "    mode=\"static\"\n",
    ")\n",
    "\n",
    "# 2. Embedding UDF\n",
    "# We load the model once globally (or use a service) to avoid reloading per row\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "@pw.udf\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    return embedder.encode(text).tolist()\n",
    "\n",
    "# 3. Vectorization\n",
    "# We compute embeddings for both Documents and Claims\n",
    "docs_with_vectors = documents.with_columns(\n",
    "    vector=get_embedding(pw.this.chunk_text),\n",
    "    book_name=pw.this.metadata[\"book_name\"]\n",
    ")\n",
    "\n",
    "claims_with_vectors = claims.with_columns(\n",
    "    vector=get_embedding(pw.this.input_text),\n",
    "    book_name=pw.this.metadata[\"book_name\"]\n",
    ")\n",
    "\n",
    "# 4. SCOPED RAG (The Critical Fix)\n",
    "# We join Claims with Docs using KNN, BUT we enforce a hard filter on book_name.\n",
    "# We retrieve TOP 30 chunks to ensure we don't miss the \"needle\".\n",
    "retrieved_context = pw.ml.index.knn_query(\n",
    "    docs_with_vectors,\n",
    "    claims_with_vectors,\n",
    "    k=30, # High context depth\n",
    "    vector_col=pw.this.vector,\n",
    "    filter=pw.this.book_name == pw.arg.book_name # <--- THE MAGIC FILTER\n",
    ")\n",
    "\n",
    "# 5. Format Context\n",
    "# Collapse the 30 retrieved chunks into a single string\n",
    "@pw.udf\n",
    "def format_context_list(texts: list[str]) -> str:\n",
    "    combined = \"\"\n",
    "    for i, text in enumerate(texts):\n",
    "        combined += f\"[{i+1}] {text}\\n\\n\"\n",
    "    return combined\n",
    "\n",
    "enriched_claims = claims_with_vectors.with_columns(\n",
    "    context=format_context_list(retrieved_context.chunk_text)\n",
    ")\n",
    "\n",
    "# 6. Final Judgment\n",
    "# Pass the Claim + Scoped Context to the LLM\n",
    "final_results = enriched_claims.with_columns(\n",
    "    llm_raw=call_judge_model(pw.this.input_text, pw.this.context)\n",
    ").with_columns(\n",
    "    verdict=parse_verdict(pw.this.llm_raw),\n",
    "    reason=parse_reason(pw.this.llm_raw)\n",
    ")\n",
    "\n",
    "# Select clean columns for output\n",
    "output_table = final_results.select(\n",
    "    pw.this.id,\n",
    "    pw.this.input_text,\n",
    "    pw.this.verdict,\n",
    "    pw.this.reason,\n",
    "    pw.this.metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6bb81",
   "metadata": {},
   "source": [
    "Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c1c78f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2062300073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write to JSONL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjsonlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Starting Pathway Pipeline... (This handles ingestion, indexing, and judging)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_table' is not defined"
     ]
    }
   ],
   "source": [
    "# Write to JSONL\n",
    "pw.io.jsonlines.write(output_table, OUTPUT_FILE)\n",
    "\n",
    "# Run the pipeline\n",
    "print(\" Starting Pathway Pipeline... (This handles ingestion, indexing, and judging)\")\n",
    "pw.run()\n",
    "print(\" Pipeline Finished. Check\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068eb2b",
   "metadata": {},
   "source": [
    "# All-In-One Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a652d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Kharagpur-Data-Science-Hackathon-2026' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AbitathaRoy/Kharagpur-Data-Science-Hackathon-2026.git\n",
    "# Then adjust your paths like: CHUNKS_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/chunks.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9bfca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pre-processing: Preparing Claims File...\n",
      " Data prepared: Kharagpur-Data-Science-Hackathon-2026/data/claims_cleaned.jsonl\n",
      " Loading Embedding Model (all-MiniLM-L6-v2)...\n",
      " Ingesting Data...\n",
      " Vectorizing & Normalizing...\n",
      " Performing Book-Scoped Similarity Search...\n",
      "  Judging...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7511583abc67424eab5c74e586586b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Pipeline Execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/beartype/_util/hint/pep/utilpeptest.py:311: BeartypeDecorHintPep585DeprecationWarning: PEP 484 type hint typing.Iterable[pathway.internals.expression.ColumnReference] deprecated by PEP 585. This hint is scheduled for removal in the first Python version released after October 5th, 2025. To resolve this, import this hint from \"beartype.typing\" rather than \"typing\". For further commentary and alternatives, see also:\n",
      "    https://beartype.readthedocs.io/en/latest/api_roar/#pep-585-deprecations\n",
      "  warn(\n",
      "WARNING:pathway_engine.connectors.monitoring:FileSystem(Kharagpur-Data-Science-Hackathon-2026/data/claims_cleaned.jsonl): Closing the data source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pathway_engine.connectors.monitoring:FileSystem(Kharagpur-Data-Science-Hackathon-2026/data/chunks.jsonl): Closing the data source\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Post-processing: Renaming 'claim_id' -> 'id'...\n",
      " Finished. Results saved to Kharagpur-Data-Science-Hackathon-2026/data/final_predictions_pathway.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathway as pw\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- 1. HARDCODED CONFIGURATION ---\n",
    "# # Load from environment or .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Removed hardcoded environment variables\n",
    "\n",
    "# Paths\n",
    "SOURCE_CLAIMS_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/claims_output.json\"\n",
    "CLEANED_CLAIMS_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/claims_cleaned.jsonl\"\n",
    "CHUNKS_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/chunks.jsonl\"\n",
    "OUTPUT_FILE = \"Kharagpur-Data-Science-Hackathon-2026/data/final_predictions_pathway.jsonl\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\" Pre-processing: Preparing Claims File...\")\n",
    "if not os.path.exists(SOURCE_CLAIMS_FILE):\n",
    "    print(f\" Error: {SOURCE_CLAIMS_FILE} not found.\")\n",
    "else:\n",
    "    with open(SOURCE_CLAIMS_FILE, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    with open(CLEANED_CLAIMS_FILE, 'w', encoding='utf-8') as f:\n",
    "        for item in raw_data:\n",
    "            if 'id' in item:\n",
    "                item['claim_id'] = item.pop('id')\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\" Data prepared: {CLEANED_CLAIMS_FILE}\")\n",
    "\n",
    "# --- 2. DEFINE PIPELINE UDFs ---\n",
    "\n",
    "print(f\" Loading Embedding Model ({EMBEDDING_MODEL})...\")\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "@pw.udf\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    if not text: \n",
    "        return [0.0] * 384\n",
    "    return embedder.encode(text).tolist()\n",
    "\n",
    "@pw.udf\n",
    "def clean_book_name(raw_name: str) -> str:\n",
    "    \"\"\"Normalizes 'data/novels/Book Name.txt' -> 'Book Name'\"\"\"\n",
    "    if not raw_name: \n",
    "        return \"\"\n",
    "    name = os.path.basename(raw_name)\n",
    "    return os.path.splitext(name)[0]\n",
    "\n",
    "@pw.udf\n",
    "def extract_book_name_from_json(metadata: dict) -> str:\n",
    "    \"\"\"Extract book_name from metadata dict and return as string\"\"\"\n",
    "    if metadata and \"book_name\" in metadata:\n",
    "        return str(metadata[\"book_name\"])\n",
    "    return \"\"\n",
    "\n",
    "@pw.udf\n",
    "def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    import math\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    norm2 = math.sqrt(sum(b * b for b in vec2))\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a Detective verifying a witness statement against the official case files.\n",
    "\n",
    "WITNESS CLAIM: \"{claim}\"\n",
    "\n",
    "OFFICIAL CASE FILES (NOVEL TEXT):\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Search the Case Files for the specific events/characters mentioned in the Claim.\n",
    "2. **Compare Details:** Look closely at names, dates, causes of death, and relationships.\n",
    "3. **Verdict Logic:**\n",
    "   - **CONTRADICT**: If the text explicitly tells a *different* story.\n",
    "   - **CONTRADICT**: If the Claim says X happened, but the Text says Y happened *instead*.\n",
    "   - **CONSISTENT**: If the Claim is supported by the text.\n",
    "   - **CONSISTENT**: If the Claim adds extra details that do *not* conflict with the text (Silence != Lie).\n",
    "\n",
    "Return JSON: {{ \"verdict\": \"consistent\" | \"contradict\", \"reason\": \"Citing specific passage [x]\" }}\n",
    "\"\"\"\n",
    "\n",
    "@pw.udf\n",
    "def call_judge_model(claim: str, context: str) -> str:\n",
    "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not api_key: \n",
    "        return json.dumps({\"verdict\": \"error\", \"reason\": \"API Key Missing\"})\n",
    "\n",
    "    client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=api_key)\n",
    "    \n",
    "    if not context or len(context) < 10:\n",
    "        return json.dumps({\"verdict\": \"consistent\", \"reason\": \"No context found.\"})\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"qwen/qwq-32b-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a logic engine. Output strict JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": JUDGE_PROMPT_TEMPLATE.format(claim=claim, context=context)}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"verdict\": \"error\", \"reason\": str(e)})\n",
    "\n",
    "@pw.udf\n",
    "def parse_verdict(llm_response: str) -> str:\n",
    "    try: \n",
    "        return json.loads(llm_response).get(\"verdict\", \"consistent\")\n",
    "    except: \n",
    "        return \"consistent\"\n",
    "\n",
    "@pw.udf\n",
    "def parse_reason(llm_response: str) -> str:\n",
    "    try: \n",
    "        return json.loads(llm_response).get(\"reason\", \"Parse Error\")\n",
    "    except: \n",
    "        return \"Parse Error\"\n",
    "\n",
    "@pw.udf\n",
    "def prepare_search_query(text: str, claims_dict: dict) -> str:\n",
    "    events = \"\"\n",
    "    if claims_dict and \"events\" in claims_dict:\n",
    "        for e in claims_dict[\"events\"]:\n",
    "            events += \" \" + e.get(\"description\", \"\")\n",
    "    return (text + events).strip()\n",
    "\n",
    "# --- 3. BUILD PIPELINE ---\n",
    "\n",
    "print(\" Ingesting Data...\")\n",
    "\n",
    "# Read chunks with correct schema\n",
    "documents = pw.io.jsonlines.read(\n",
    "    CHUNKS_FILE,\n",
    "    schema=pw.schema_from_dict({\n",
    "        \"chunk_text\": str,\n",
    "        \"book_name\": str,\n",
    "        \"diff\": int,\n",
    "        \"time\": int\n",
    "    }),\n",
    "    mode=\"static\"\n",
    ")\n",
    "\n",
    "# Read claims\n",
    "claims = pw.io.jsonlines.read(\n",
    "    CLEANED_CLAIMS_FILE,\n",
    "    schema=pw.schema_from_dict({\n",
    "        \"claim_id\": str,\n",
    "        \"input_text\": str,\n",
    "        \"metadata\": dict,\n",
    "        \"claims\": dict\n",
    "    }),\n",
    "    mode=\"static\"\n",
    ")\n",
    "\n",
    "print(\" Vectorizing & Normalizing...\")\n",
    "docs_with_vectors = documents.with_columns(\n",
    "    vector=get_embedding(pw.this.chunk_text),\n",
    "    clean_book_name=clean_book_name(pw.this.book_name)\n",
    ")\n",
    "\n",
    "# FIX: Extract book_name from metadata dict and convert to string\n",
    "claims_with_vectors = claims.with_columns(\n",
    "    search_text=prepare_search_query(pw.this.input_text, pw.this.claims),\n",
    "    book_name_str=extract_book_name_from_json(pw.this.metadata)\n",
    ").with_columns(\n",
    "    vector=get_embedding(pw.this.search_text)\n",
    ")\n",
    "\n",
    "print(\" Performing Book-Scoped Similarity Search...\")\n",
    "\n",
    "# Step 1: Join on book_name to filter documents (both are now strings)\n",
    "joined = claims_with_vectors.join(\n",
    "    docs_with_vectors,\n",
    "    claims_with_vectors.book_name_str == docs_with_vectors.clean_book_name,\n",
    "    id=claims_with_vectors.id\n",
    ").select(\n",
    "    claim_id=claims_with_vectors.claim_id,\n",
    "    input_text=claims_with_vectors.input_text,\n",
    "    metadata=claims_with_vectors.metadata,\n",
    "    book_name_str=claims_with_vectors.book_name_str,\n",
    "    claim_vector=claims_with_vectors.vector,\n",
    "    chunk_text=docs_with_vectors.chunk_text,\n",
    "    chunk_vector=docs_with_vectors.vector\n",
    ")\n",
    "\n",
    "# Step 2: Calculate similarity scores\n",
    "joined_with_scores = joined.with_columns(\n",
    "    similarity=cosine_similarity(pw.this.claim_vector, pw.this.chunk_vector)\n",
    ")\n",
    "\n",
    "# Step 3: Group by claim and collect chunks with scores\n",
    "grouped = joined_with_scores.groupby(\n",
    "    pw.this.claim_id,\n",
    "    pw.this.input_text,\n",
    "    pw.this.metadata,\n",
    "    pw.this.book_name_str\n",
    ").reduce(\n",
    "    claim_id=pw.this.claim_id,\n",
    "    input_text=pw.this.input_text,\n",
    "    metadata=pw.this.metadata,\n",
    "    book_name_str=pw.this.book_name_str,\n",
    "    # Collect all chunks with their similarity scores\n",
    "    all_chunks=pw.reducers.tuple(pw.this.chunk_text),\n",
    "    all_scores=pw.reducers.tuple(pw.this.similarity)\n",
    ")\n",
    "\n",
    "# Step 4: Sort and format top chunks in Python\n",
    "@pw.udf\n",
    "def get_top_chunks_formatted(chunks: tuple, scores: tuple) -> str:\n",
    "    \"\"\"Sort chunks by score and format top 15\"\"\"\n",
    "    if not chunks or not scores:\n",
    "        return \"\"\n",
    "    \n",
    "    # Combine chunks with scores\n",
    "    combined = list(zip(chunks, scores))\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Format top 15\n",
    "    result = \"\"\n",
    "    for i, (chunk_text, score) in enumerate(combined[:15]):\n",
    "        result += f\"[{i+1}] {chunk_text}\\n\\n\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"  Judging...\")\n",
    "final_results = grouped.with_columns(\n",
    "    context=get_top_chunks_formatted(pw.this.all_chunks, pw.this.all_scores)\n",
    ").with_columns(\n",
    "    llm_raw=call_judge_model(pw.this.input_text, pw.this.context)\n",
    ").with_columns(\n",
    "    verdict=parse_verdict(pw.this.llm_raw),\n",
    "    reason=parse_reason(pw.this.llm_raw)\n",
    ")\n",
    "\n",
    "output_table = final_results.select(\n",
    "    claim_id=pw.this.claim_id,\n",
    "    input_text=pw.this.input_text,\n",
    "    verdict=pw.this.verdict,\n",
    "    reason=pw.this.reason,\n",
    "    metadata=pw.this.metadata\n",
    ")\n",
    "\n",
    "# --- 4. EXECUTE ---\n",
    "print(\" Starting Pipeline Execution...\")\n",
    "pw.io.jsonlines.write(output_table, OUTPUT_FILE)\n",
    "pw.run()\n",
    "\n",
    "# --- 5. POST-PROCESSING ---\n",
    "print(\" Post-processing: Renaming 'claim_id' -> 'id'...\")\n",
    "temp_output = []\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'claim_id' in data:\n",
    "                    data['id'] = data.pop('claim_id')\n",
    "                temp_output.append(data)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        for item in temp_output:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\" Finished. Results saved to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\" Error: Output file not created. Check logs for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ab9249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pathway\n",
      "Version: 0.28.0\n",
      "Summary: Pathway is a data processing framework which takes care of streaming data updates for you.\n",
      "Home-page: https://pathway.com/\n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: aiohttp, aiohttp-cors, async-lru, beartype, boto3, click, deltalake, diskcache, fs, geopy, gitpython, google-api-python-client, google-cloud-bigquery, google-cloud-pubsub, h3, jmespath, jupyter-bokeh, networkx, numpy, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-sdk, pandas, panel, pyarrow, pydantic, python-sat, requests, rich, scikit-learn, shapely, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97281a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
